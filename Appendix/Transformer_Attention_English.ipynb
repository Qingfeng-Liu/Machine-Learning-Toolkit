{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 📘 Introduction to Transformer and Attention Mechanism\n",
    "\n",
    "This notebook will walk you through:\n",
    "- What is Attention?\n",
    "- Self-Attention Mechanism\n",
    "- The Transformer Architecture\n",
    "- Multi-Head Attention\n",
    "- Position-wise Feed-Forward Networks\n",
    "- Positional Encoding\n",
    "- Code Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🔍 1. What is Attention?\n",
    "\n",
    "In the context of deep learning, **attention** is a technique that allows models to dynamically focus on relevant parts of the input sequence.\n",
    "\n",
    "Consider translating:\n",
    "> \"The animal didn't cross the street because it was too tired.\"\n",
    "\n",
    "When deciding what \"it\" refers to, the model must **attend** to the word \"animal\".\n",
    "\n",
    "### The Core Idea:\n",
    "Instead of encoding the entire sentence into a fixed vector (as in traditional RNN/Seq2Seq), attention allows the model to **weigh** the importance of each word in the sequence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ✨ 2. Scaled Dot-Product Attention (Enhanced Explanation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Core Concept Visualization\n",
    "\n",
    "Imagine you're a teacher grading essays:\n",
    "- **Query (Q)**: Your grading rubric (what you're looking for)\n",
    "- **Key (K)**: Each essay's thesis statement\n",
    "- **Value (V)**: The actual essay content\n",
    "\n",
    "```mermaid\n",
    "graph LR\n",
    "    Q[Query] -->|Compare| K[Key]\n",
    "    K -->|Generate| W[Weights]\n",
    "    W -->|Weighted Sum| V[Value]\n",
    "    V --> O[Output]\n",
    "```\n",
    "\n",
    "### Step-by-Step Calculation\n",
    "\n",
    "1. **Similarity Scores**: \n",
    "   ```python\n",
    "   raw_scores = Q @ K.T  # Matrix multiplication\n",
    "   ```\n",
    "   \n",
    "2. **Scaling**:\n",
    "   ```python\n",
    "   scaled_scores = raw_scores / sqrt(d_k)  # d_k = dimension of keys\n",
    "   ```\n",
    "   \n",
    "3. **Softmax Normalization**:\n",
    "   ```python\n",
    "   attention_weights = softmax(scaled_scores)\n",
    "   ```\n",
    "   \n",
    "4. **Context Vector**:\n",
    "   ```python\n",
    "   output = attention_weights @ V\n",
    "   ```\n",
    "\n",
    "### Why Scaling Matters\n",
    "Without scaling (`1/sqrt(d_k)`), when dimensions are large:\n",
    "- Dot products grow extremely large\n",
    "- Softmax gradients become vanishingly small\n",
    "- Model can't learn effectively\n",
    "\n",
    "### Complete Formula\n",
    "\n",
    "$$\n",
    "\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🔁 3. Self-Attention (Detailed Breakdown)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Self-Attention vs Regular Attention\n",
    "\n",
    "| Feature          | Self-Attention         | Regular Attention      |\n",
    "|------------------|-----------------------|-----------------------|\n",
    "| Query Source     | Input sequence itself | External (e.g., decoder) |\n",
    "| Key/Value Source | Input sequence itself | External source       |\n",
    "| Directionality   | Bidirectional         | Often unidirectional  |\n",
    "\n",
    "### Key Properties\n",
    "\n",
    "1. **Long-Range Dependencies**:\n",
    "   ```text\n",
    "   Sentence: \"The cat, which was hungry despite eating recently, meowed loudly.\"\n",
    "   Relation: \"cat\" ↔ \"meowed\" (distant but directly connected)\n",
    "   ```\n",
    "   \n",
    "2. **Parallel Computation**:\n",
    "   All position pairs computed simultaneously (unlike RNNs)\n",
    "   \n",
    "3. **Contextual Encoding**:\n",
    "   Each word's representation changes based on entire context\n",
    "\n",
    "### Computation Process\n",
    "\n",
    "```python\n",
    "# Input embeddings (n × d_model)\n",
    "X = [word1_embed, word2_embed, ...]  \n",
    "\n",
    "# Learnable projections\n",
    "Q = X @ W_Q  # (n × d_k)\n",
    "K = X @ W_K  # (n × d_k)\n",
    "V = X @ W_V  # (n × d_v)\n",
    "\n",
    "# Self-attention calculation\n",
    "attention = softmax((Q @ K.T)/sqrt(d_k)) @ V  # (n × d_v)\n",
    "```\n",
    "\n",
    "### Practical Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Sample sentence embeddings (3 words, 4 dimensions each)\n",
    "X = torch.tensor([\n",
    "    [1.0, 0.2, -0.5, 0.3],  # Word 1 (\"The\")\n",
    "    [0.5, 1.2, 0.1, -0.7], # Word 2 (\"cat\")\n",
    "    [-0.3, 0.8, 1.1, 0.4]   # Word 3 (\"meowed\")\n",
    "], dtype=torch.float32)\n",
    "\n",
    "# Projection matrices (randomly initialized)\n",
    "W_Q = torch.randn(4, 3)\n",
    "W_K = torch.randn(4, 3)\n",
    "W_V = torch.randn(4, 2)\n",
    "\n",
    "# Compute Q, K, V\n",
    "Q = X @ W_Q\n",
    "K = X @ W_K\n",
    "V = X @ W_V\n",
    "\n",
    "# Self-attention calculation\n",
    "d_k = Q.size(-1)\n",
    "scores = (Q @ K.T) / torch.sqrt(torch.tensor(d_k))\n",
    "weights = F.softmax(scores, dim=-1)\n",
    "output = weights @ V\n",
    "\n",
    "print(\"Attention Weights:\\n\", weights)\n",
    "print(\"\\nOutput (Contextual Embeddings):\\n\", output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretation\n",
    "- **Attention Weights**: Shows how much each word attends to others\n",
    "- **Output**: New embeddings where each word contains information from relevant context words\n",
    "\n",
    "> Note: Real implementations use multi-head attention and masking for decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🧠 4. Transformer Architecture (Encoder-Decoder)\n",
    "\n",
    "The original Transformer consists of:\n",
    "\n",
    "### Encoder Block:\n",
    "- Multi-head self-attention\n",
    "- Feed-forward layer\n",
    "- Residual + LayerNorm\n",
    "\n",
    "### Decoder Block:\n",
    "- Masked multi-head self-attention\n",
    "- Multi-head attention over encoder output\n",
    "- Feed-forward layer\n",
    "- Residual + LayerNorm\n",
    "\n",
    "```mermaid\n",
    "graph TD\n",
    "    A[Input] --> B[Positional Encoding]\n",
    "    B --> C[Encoder Blocks]\n",
    "    C --> D[Context Vector]\n",
    "    D --> E[Decoder Blocks]\n",
    "    E --> F[Output]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🧩 5. Multi-Head Attention\n",
    "\n",
    "Instead of one single attention function, the Transformer uses **multiple attention heads**:\n",
    "\n",
    "$$\n",
    "\\text{MultiHead}(Q, K, V) = \\text{Concat}(head_1, ..., head_h)W^O\n",
    "$$\n",
    "\n",
    "where each head is:\n",
    "$$\n",
    "head_i = \\text{Attention}(QW_i^Q, KW_i^K, VW_i^V)\n",
    "$$\n",
    "\n",
    "### Why Multiple Heads?\n",
    "- Each head learns different attention patterns\n",
    "- Example heads might focus on:\n",
    "  - Positional relationships\n",
    "  - Syntactic features\n",
    "  - Semantic relationships\n",
    "- Combines benefits of CNN's multiple filters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🔄 6. Feed-Forward Layer\n",
    "\n",
    "Each position in the sequence is passed through the same feed-forward network:\n",
    "\n",
    "$$\n",
    "\\text{FFN}(x) = \\text{ReLU}(xW_1 + b_1)W_2 + b_2\n",
    "$$\n",
    "\n",
    "### Key Properties:\n",
    "- Applied independently to each position\n",
    "- Usually expands dimension then reduces (e.g., 512 → 2048 → 512)\n",
    "- Provides additional non-linearity\n",
    "- Sometimes called \"position-wise\" FFN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🌀 7. Positional Encoding\n",
    "\n",
    "Since the Transformer has no recurrence, it adds **positional encodings** to input embeddings:\n",
    "\n",
    "$$\n",
    "PE_{(pos, 2i)} = \\sin\\left(\\frac{pos}{10000^{2i/d_{model}}}\\right)\n",
    "$$\n",
    "$$\n",
    "PE_{(pos, 2i+1)} = \\cos\\left(\\frac{pos}{10000^{2i/d_{model}}}\\right)\n",
    "$$\n",
    "\n",
    "### Visualized Pattern:\n",
    "<img src=\"https://jalammar.github.io/images/t/transformer_positional_encoding_example.png\" width=\"400\">\n",
    "\n",
    "### Why This Works:\n",
    "- Unique encoding for each position\n",
    "- Relative positions can be linearly attended to\n",
    "- Stable across sequence lengths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 💡 8. Complete Self-Attention Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, d_model, d_k):\n",
    "        super().__init__()\n",
    "        self.W_Q = nn.Linear(d_model, d_k)\n",
    "        self.W_K = nn.Linear(d_model, d_k)\n",
    "        self.W_V = nn.Linear(d_model, d_k)\n",
    "        self.d_k = d_k\n",
    "        \n",
    "    def forward(self, X):\n",
    "        Q = self.W_Q(X)  # (batch_size, seq_len, d_k)\n",
    "        K = self.W_K(X)  # (batch_size, seq_len, d_k)\n",
    "        V = self.W_V(X)  # (batch_size, seq_len, d_v)\n",
    "        \n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
    "        weights = torch.softmax(scores, dim=-1)\n",
    "        output = torch.matmul(weights, V)\n",
    "        \n",
    "        return output, weights\n",
    "\n",
    "# Example usage\n",
    "d_model = 512\n",
    "d_k = 64\n",
    "batch_size = 4\n",
    "seq_len = 10\n",
    "\n",
    "attention = SelfAttention(d_model, d_k)\n",
    "X = torch.rand(batch_size, seq_len, d_model)\n",
    "output, attn_weights = attention(X)\n",
    "\n",
    "print(f\"Input shape: {X.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(f\"Attention weights shape: {attn_weights.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🏁 Summary\n",
    "\n",
    "- **Attention Mechanisms**: Enable dynamic focus on relevant input parts\n",
    "- **Self-Attention**: Captures all pairwise relationships in a sequence\n",
    "- **Transformer Architecture**:\n",
    "  - Encoder: Processes input with self-attention and FFN\n",
    "  - Decoder: Generates output with masked self-attention\n",
    "- **Key Innovations**:\n",
    "  - Multi-head attention\n",
    "  - Positional encodings\n",
    "  - Residual connections\n",
    "- **Impact**: Foundation for modern models like BERT, GPT, T5, etc.\n",
    "\n",
    "Next Steps:\n",
    "- Experiment with multi-head attention\n",
    "- Implement a full Transformer\n",
    "- Explore variants (Sparse Attention, Linear Attention)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 }
}