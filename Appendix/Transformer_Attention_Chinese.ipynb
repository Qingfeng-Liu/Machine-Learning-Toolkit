{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 📘 Transformer与注意力机制详解"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🔍 1. 什么是注意力机制？\n",
    "\n",
    "在深度学习中，**注意力机制**让模型能够动态关注输入序列的相关部分。\n",
    "\n",
    "例如翻译句子：\n",
    "> \"这只动物没有过马路，因为它太累了\"\n",
    "\n",
    "当判断\"它\"指代什么时，模型需要**关注**\"动物\"这个词。\n",
    "\n",
    "### 核心思想：\n",
    "不同于传统RNN/Seq2Seq将整个句子编码为固定向量，注意力机制让模型可以**动态权衡**序列中每个词的重要性。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ✨ 2. 缩放点积注意力（图解版）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 生活化类比\n",
    "\n",
    "想象你在书店找书：\n",
    "- **Query (Q)**：你的需求（\"想找Python编程书\"）\n",
    "- **Key (K)**：每本书的目录标题\n",
    "- **Value (V)**：书籍的实际内容\n",
    "\n",
    "```mermaid\n",
    "graph LR\n",
    "    Q[查询] -->|匹配| K[关键词]\n",
    "    K -->|生成| W[权重]\n",
    "    W -->|加权求和| V[值]\n",
    "    V --> O[输出]\n",
    "```\n",
    "\n",
    "### 计算四部曲\n",
    "\n",
    "1. **相似度计算**：\n",
    "   ```python\n",
    "   原始分数 = Q @ K.T  # 矩阵乘法\n",
    "   ```\n",
    "   \n",
    "2. **缩放处理**：\n",
    "   ```python\n",
    "   缩放分数 = 原始分数 / sqrt(d_k)  # d_k=键向量维度\n",
    "   ```\n",
    "   \n",
    "3. **权重归一化**：\n",
    "   ```python\n",
    "   注意力权重 = softmax(缩放分数)  # 得到0-1之间的权重\n",
    "   ```\n",
    "   \n",
    "4. **加权求和**：\n",
    "   ```python\n",
    "   输出 = 注意力权重 @ V  # 最终上下文向量\n",
    "   ```\n",
    "\n",
    "### 为什么需要缩放？\n",
    "当维度`d_k`较大时，点积结果会变得极大，导致softmax梯度消失。缩放保持梯度稳定。\n",
    "\n",
    "### 数学公式\n",
    "\n",
    "$$\n",
    "\\text{注意力}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🔁 3. 自注意力机制（完整解析）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 与传统注意力的区别\n",
    "\n",
    "| 特性          | 自注意力              | 传统注意力            |\n",
    "|---------------|----------------------|----------------------|\n",
    "| 查询来源       | 输入序列自身          | 外部（如解码器）      |\n",
    "| 键/值来源      | 输入序列自身          | 外部源                |\n",
    "| 方向性        | 双向                 | 通常单向              |\n",
    "\n",
    "### 三大核心优势\n",
    "\n",
    "1. **捕捉长距离依赖**：\n",
    "   ```text\n",
    "   句子：\"这只猫，尽管刚吃过饭还是很饿，大声喵喵叫\"\n",
    "   关系：\"猫\" ↔ \"喵喵叫\"（距离远但直接关联）\n",
    "   ```\n",
    "   \n",
    "2. **并行计算**：\n",
    "   所有位置对可以同时计算（不同于RNN的顺序处理）\n",
    "   \n",
    "3. **上下文编码**：\n",
    "   每个词的表示会根据全文语境动态变化\n",
    "\n",
    "### 计算流程\n",
    "\n",
    "```python\n",
    "# 输入词向量 (n × 模型维度)\n",
    "X = [词1向量, 词2向量, ...]  \n",
    "\n",
    "# 可学习的投影矩阵\n",
    "Q = X @ W_Q  # (n × d_k)\n",
    "K = X @ W_K  # (n × d_k)\n",
    "V = X @ W_V  # (n × d_v)\n",
    "\n",
    "# 自注意力计算\n",
    "注意力 = softmax((Q @ K.T)/sqrt(d_k)) @ V  # (n × d_v)\n",
    "```\n",
    "\n",
    "### 实际代码演示"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# 示例句子编码（3个词，每个词4维）\n",
    "X = torch.tensor([\n",
    "    [1.0, 0.2, -0.5, 0.3],  # 词1（\"猫\"）\n",
    "    [0.5, 1.2, 0.1, -0.7],  # 词2（\"饿\"）\n",
    "    [-0.3, 0.8, 1.1, 0.4]   # 词3（\"叫\"）\n",
    "], dtype=torch.float32)\n",
    "\n",
    "# 随机初始化投影矩阵（实际训练中自动学习）\n",
    "W_Q = torch.randn(4, 3)\n",
    "W_K = torch.randn(4, 3)\n",
    "W_V = torch.randn(4, 2)\n",
    "\n",
    "# 计算Q,K,V\n",
    "Q = X @ W_Q\n",
    "K = X @ W_K\n",
    "V = X @ W_V\n",
    "\n",
    "# 自注意力计算\n",
    "d_k = Q.size(-1)\n",
    "分数 = (Q @ K.T) / torch.sqrt(torch.tensor(d_k))\n",
    "权重 = F.softmax(分数, dim=-1)\n",
    "输出 = 权重 @ V\n",
    "\n",
    "print(\"注意力权重矩阵：\\n\", 权重)\n",
    "print(\"\\n输出（上下文感知的词向量）：\\n\", 输出)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 结果解读\n",
    "- **注意力权重**：显示词与词之间的关注强度\n",
    "- **输出向量**：每个词的新表示都融合了相关上下文信息\n",
    "\n",
    "> 注意：实际应用会使用多头注意力（Multi-Head Attention）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🧠 4. Transformer架构（编码器-解码器）\n",
    "\n",
    "原始Transformer组成：\n",
    "\n",
    "### 编码器模块：\n",
    "- 多头自注意力\n",
    "- 前馈神经网络\n",
    "- 残差连接 + 层归一化\n",
    "\n",
    "### 解码器模块：\n",
    "- 掩码多头自注意力\n",
    "- 编码器-解码器注意力\n",
    "- 前馈神经网络\n",
    "- 残差连接 + 层归一化\n",
    "\n",
    "```mermaid\n",
    "graph TD\n",
    "    A[输入] --> B[位置编码]\n",
    "    B --> C[编码器堆叠]\n",
    "    C --> D[上下文向量]\n",
    "    D --> E[解码器堆叠]\n",
    "    E --> F[输出]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🧩 5. 多头注意力机制\n",
    "\n",
    "Transformer使用**多组注意力头**而非单个：\n",
    "\n",
    "$$\n",
    "\\text{多头注意力}(Q, K, V) = \\text{拼接}(头_1, ..., 头_h)W^O\n",
    "$$\n",
    "\n",
    "其中每个头的计算：\n",
    "$$\n",
    "头_i = \\text{注意力}(QW_i^Q, KW_i^K, VW_i^V)\n",
    "$$\n",
    "\n",
    "### 多头设计的优势\n",
    "- 不同头学习不同关注模式\n",
    "- 例如可能分别关注：\n",
    "  - 语法结构\n",
    "  - 语义关系\n",
    "  - 指代关联\n",
    "- 类似CNN中多滤波器的效果"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🔄 6. 逐位置前馈网络\n",
    "\n",
    "序列每个位置独立通过相同的前馈网络：\n",
    "\n",
    "$$\n",
    "\\text{FFN}(x) = \\text{ReLU}(xW_1 + b_1)W_2 + b_2\n",
    "$$\n",
    "\n",
    "### 关键特性：\n",
    "- 独立处理每个位置\n",
    "- 通常先扩维再降维（如512 → 2048 → 512）\n",
    "- 提供额外非线性变换\n",
    "- 又称\"位置感知\"前馈网络"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🌀 7. 位置编码\n",
    "\n",
    "因Transformer没有循环结构，需添加**位置编码**到输入嵌入：\n",
    "\n",
    "$$\n",
    "PE_{(位置, 2i)} = \\sin\\left(\\frac{位置}{10000^{2i/d_{模型}}}\\right)\n",
    "$$\n",
    "$$\n",
    "PE_{(位置, 2i+1)} = \\cos\\left(\\frac{位置}{10000^{2i/d_{模型}}}\\right)\n",
    "$$\n",
    "\n",
    "### 可视化模式：\n",
    "<img src=\"https://jalammar.github.io/images/t/transformer_positional_encoding_example.png\" width=\"400\">\n",
    "\n",
    "### 设计原理：\n",
    "- 每个位置有唯一编码\n",
    "- 相对位置可被线性关注\n",
    "- 对任意长度序列都适用"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 💡 8. 自注意力完整实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class 自注意力(nn.Module):\n",
    "    def __init__(self, 模型维度, 键维度):\n",
    "        super().__init__()\n",
    "        self.W_Q = nn.Linear(模型维度, 键维度)  # 查询变换\n",
    "        self.W_K = nn.Linear(模型维度, 键维度)  # 键变换\n",
    "        self.W_V = nn.Linear(模型维度, 键维度)  # 值变换\n",
    "        self.键维度 = 键维度\n",
    "        \n",
    "    def forward(self, X):\n",
    "        Q = self.W_Q(X)  # (批大小, 序列长, 键维度)\n",
    "        K = self.W_K(X)  # (批大小, 序列长, 键维度)\n",
    "        V = self.W_V(X)  # (批大小, 序列长, 键维度)\n",
    "        \n",
    "        分数 = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.键维度)\n",
    "        权重 = torch.softmax(分数, dim=-1)\n",
    "        输出 = torch.matmul(权重, V)\n",
    "        \n",
    "        return 输出, 权重\n",
    "\n",
    "# 使用示例\n",
    "模型维度 = 512\n",
    "键维度 = 64\n",
    "批大小 = 4\n",
    "序列长 = 10\n",
    "\n",
    "注意力层 = 自注意力(模型维度, 键维度)\n",
    "输入 = torch.rand(批大小, 序列长, 模型维度)\n",
    "输出, 注意力权重 = 注意力层(输入)\n",
    "\n",
    "print(f\"输入维度: {输入.shape}\")\n",
    "print(f\"输出维度: {输出.shape}\")\n",
    "print(f\"注意力权重维度: {注意力权重.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🏁 总结\n",
    "\n",
    "- **注意力机制**：动态聚焦输入相关部分\n",
    "- **自注意力**：捕捉序列元素间的两两关系\n",
    "- **Transformer架构**：\n",
    "  - 编码器：通过自注意力处理输入\n",
    "  - 解码器：生成式预测输出\n",
    "- **关键创新**：\n",
    "  - 多头注意力\n",
    "  - 位置编码\n",
    "  - 残差连接\n",
    "- **影响**：BERT、GPT等模型的基石\n",
    "\n",
    "后续方向：\n",
    "- 实验多头注意力\n",
    "- 实现完整Transformer\n",
    "- 探索变体（稀疏注意力、线性注意力）"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 }
}