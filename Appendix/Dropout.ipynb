{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dropout Mechanism Explained"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Core Concept\n",
    "Dropout is a regularization technique that randomly deactivates neurons during training to prevent overfitting. Key properties:\n",
    "- **Training Phase**: Randomly mask neurons with probability `p` and scale activations by `1/(1-p)`\n",
    "- **Inference Phase**: Use all neurons but multiply outputs by `(1-p)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Mathematical Formulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Phase\n",
    "For neuron output `a`:\n",
    "1. Generate mask `m ~ Bernoulli(1-p)`\n",
    "   - `P(m=1) = 1-p` (keep)\n",
    "   - `P(m=0) = p` (drop)\n",
    "2. Apply dropout:\n",
    "   $$a_{\\text{train}} = m \\cdot \\frac{a}{1-p}$$\n",
    "3. Expected output:\n",
    "   $$\\mathbb{E}[a_{\\text{train}}}] = a$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference Phase\n",
    "To maintain consistent output scale:\n",
    "$$a_{\\text{test}} = a \\cdot (1-p)$$\n",
    "\n",
    "**Derivation**:\n",
    "$$\\mathbb{E}[a_{\\text{train}}}] = a = \\mathbb{E}[a_{\\text{test}}}] \\cdot \\frac{1}{1-p}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Numerical Example (p=0.6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Phase   | Calculation                     | Example Output (a=1) | Expected Value |\n",
    "|---------|----------------------------------|----------------------|----------------|\n",
    "| Training | 40% chance: 1/0.4=2.5<br>60% chance: 0 | Possible outputs: 2.5 or 0 | 1.0            |\n",
    "| Testing  | 1Ã—0.4=0.4                        | Fixed output: 0.4     | 0.4            |\n",
    "\n",
    "**Consistency Check**:\n",
    "$$2.5 \\times 0.4 + 0 \\times 0.6 = 1.0 = 0.4 \\times \\frac{1}{0.4}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. PyTorch Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Initialize dropout with p=0.6\n",
    "dropout = nn.Dropout(p=0.6)\n",
    "\n",
    "# Create sample input\n",
    "x = torch.ones(10)\n",
    "print(\"Original input:\", x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training mode\n",
    "y_train = dropout(x)\n",
    "print(\"Training output:\", y_train)\n",
    "\n",
    "# Note: About 60% of values will be 0, others scaled by 1/(1-0.6)=2.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference mode\n",
    "dropout.eval()\n",
    "y_test = dropout(x)\n",
    "print(\"Inference output:\", y_test)\n",
    "\n",
    "# All values scaled by (1-p)=0.4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Backpropagation Behavior\n",
    "- Gradients are only backpropagated through active neurons\n",
    "- Gradient magnitudes are scaled by `1/(1-p)` for kept neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Backprop example\n",
    "x = torch.randn(5, requires_grad=True)\n",
    "dropout.train()\n",
    "y = dropout(x)\n",
    "loss = y.sum()\n",
    "loss.backward()\n",
    "\n",
    "print(\"Input gradients:\", x.grad)\n",
    "# Note: Gradients for dropped neurons are 0, others scaled up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Design Implications\n",
    "- **Training**: Forces redundant representations\n",
    "- **Inference**: Equivalent to model averaging\n",
    "- **Scaling**: Maintains expected activation magnitudes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Special Cases\n",
    "- **Input Layer Dropout**: Typically use smaller p (0.1-0.2)\n",
    "- **With BatchNorm**: May interfere with statistics - use carefully"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Mathematical Essence\n",
    "Implicit model averaging:\n",
    "$$\\mathbb{E}_{\\text{train}}[y] = \\frac{1}{M} \\sum_{i=1}^M y_{\\text{submodel}_i} = y_{\\text{full}} \\cdot (1-p)$$\n",
    "where M is the number of possible submodels."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
